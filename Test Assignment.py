# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

!pip install requests
!pip install html5lib
!pip install bs4

import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.corpus import cmudict
from textblob import TextBlob
from nltk.tag import pos_tag

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('cmudict')
nltk.download('averaged_perceptron_tagger')

dataset = pd.read_excel('Input.xlsx')

def remove_html_tag(text):
  pattern = re.compile('<.*?>')
  return pattern.sub(r'', text)

file = open("URL_ID.txt",'w')
for url in dataset['URL']:
  r = requests.get(url)
  HtmlContent = r.content
  soup = BeautifulSoup(HtmlContent,'html.parser')
  title = str(soup.find_all('title'))
  paras = str(soup.find_all('p'))
  heading1 = str(soup.find_all('h1'))
  heading3 = str(soup.find_all('h3'))
  Sub_head = str(soup.find_all('ol'))
  file.write(remove_html_tag(title))
  file.write(remove_html_tag(paras))
  file.write(remove_html_tag(heading1))
  file.write(remove_html_tag(Sub_head))
  file.write(remove_html_tag(heading3))


file.close

#TEXT FETCHING
    def fetch_text_from_file(file_path):
        try:
            # Open file in read mode
            with open(file_path, 'r') as file:
                # Read entire file content into a string
                file_contents = file.read()
            return file_contents
        except FileNotFoundError:
            print(f"File '{file_path}' not found.")
            return None

    #ERASING FILE
    def erase_file_contents(file_path):
        # Open the file in write mode, which clears existing contents
        with open(file_path, 'w') as file:
            pass

    #POSTIVE SCORE
    def get_positive_score(text):
        blob = TextBlob(text)
        positive_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity > 0)
        return positive_score

    #NEGATIVE SCORE
    def get_negative_score(text):
        blob = TextBlob(text)
        negative_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity < 0)
        return negative_score

    #POLARITY SCORE
    def get_polarity_score(text):
        blob = TextBlob(text)
        polarity_score = blob.sentiment.polarity
        return polarity_score

    #SUBJECTIVITY SCORE
    def get_subjectivity_score(text):
        blob = TextBlob(text)
        subjectivity_score = blob.sentiment.subjectivity
        return subjectivity_score

    #TOTAL SENTENCES
    def get_total_sentence(text):
        sentences = sent_tokenize(text)
        total_sentences = len(sentences)
        return total_sentences

    #TOTAL WORDS
    def get_total_words(text):
        # Tokenize text into sentences
        sentences = sent_tokenize(text)
        # Calculate total number of words and sentences
        total_words = 0
        total_sentences = get_total_sentence(text)

        # Count words in each sentence
        for sentence in sentences:
            words = word_tokenize(sentence)
            total_words += len(words)
        return total_words

    #AVERAGE SENTENCE LENGTH
    def get_average_sentence_length(text):
        sentences = sent_tokenize(text)
        total_sentences = len(sentences)
        if total_sentences > 0:
            avg_sentence_length = get_total_words(text) / total_sentences
        else:
            avg_sentence_length = 0

        return avg_sentence_length

    #PERCENTAGE OF COMPLEX WORDS
    def percentage_complex_words(text):
      complex_count, total_count = count_complex_words(text)

      if total_count > 0:
          percentage = (complex_count / total_count) * 100
      else:
          percentage = 0

      return percentage

    #FOG INDEX
    def calculate_fog_index(text):
        # Tokenize the text into sentences and words
        sentences = sent_tokenize(text)
        words = word_tokenize(text)

        # Remove stopwords
        stop_words = set(stopwords.words('english'))
        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

        total_words = len(words)
        total_sentences = len(sentences)

        complex_words = 0

        # Count complex words (words with 3 or more syllables)
        for word in words:
            syllables = syllable_count(word)
            if syllables >= 3:
                complex_words += 1

        # Calculate average words per sentence
        average_words_per_sentence = total_words / total_sentences

        # Calculate percentage of complex words
        percentage_complex_words = (complex_words / total_words) * 100

        # Calculate Fog Index
        fog_index = 0.4 * (average_words_per_sentence + percentage_complex_words)

        return fog_index


    #SYLLABLES COUNT
    # Function to count syllables in a word using CMU Pronouncing Dictionary
    def syllable_count(word):
        try:
            return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]
        except KeyError:
            # If word is not found in CMU dictionary, return a basic estimate
            return max(1, len(word) // 3)  # Simple heuristic: 1 syllable per 3 characters

    #AVERAGE WORDS PER SENTENCE
    def average_words_per_sentence(text):
        # Calculate average
        if get_total_sentence(text) > 0:
            average_words = get_total_words(text) / get_total_sentence(text)
        else:
            average_words = 0

        return average_words

    #COUNT COMPLEX WORDS
    from textstat import syllable_count

    def count_complex_words(text):
        # Tokenize the text into words
        words = word_tokenize(text)

        # Filter out stopwords since they are usually not complex words
        stop_words = set(stopwords.words('english'))
        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

        complex_word_count = 0

        for word in words:
            # Check if the word meets the complexity criterion
            syllables = syllable_count(word)

            # Define your complexity threshold here, e.g., words with more than 2 syllables
            if syllables >= 3:  # Adjust this threshold as needed
                complex_word_count += 1

        return complex_word_count, len(words)


    #WORD COUNT
    def get_word_count(text):
        # Split the text into words using whitespace as delimiter
        words = text.split()
        # Count the number of words
        word_count = len(words)
        return word_count

    #SYLLABLES PER WORD
    # Function to calculate syllables per word in a text
    def get_syllables_per_word(text):
        words = word_tokenize(text)
        syllable_count = 0

        for word in words:
            syllable_count += count_syllables(word)

        # Calculate average syllables per word
        if get_total_words(text) > 0:
            syllables_per_word = syllable_count / get_total_words(text)
        else:
            syllables_per_word = 0

        return syllables_per_word

    #PERSONAL PRONOUNS
    def find_personal_pronouns(text):
        personal_pronouns = ['i', 'me', 'my', 'mine', 'myself',
                            'you', 'your', 'yours', 'yourself', 'yourselves',
                            'he', 'him', 'his', 'himself',
                            'she', 'her', 'hers', 'herself',
                            'it', 'its', 'itself',
                            'we', 'us', 'our', 'ours', 'ourselves',
                            'they', 'them', 'their', 'theirs', 'themselves']

        # Tokenize the text into sentences and words
        sentences = sent_tokenize(text)
        words = [word_tokenize(sentence) for sentence in sentences]

        # Tag parts of speech for each word
        tagged_words = [pos_tag(word) for word in words]

        # Extract personal pronouns from tagged words
        personal_pronouns_found = []
        for sentence in tagged_words:
            for word, tag in sentence:
                if word.lower() in personal_pronouns:
                    personal_pronouns_found.append(word)

        return personal_pronouns_found


    #AVERAGE WORD LENGTH
    def average_word_length(text):
        # Tokenize the text into words
        words = text.split()

        # Calculate total characters
        total_characters = sum(len(word) for word in words)

        # Calculate average word length
        if get_total_words(text) > 0:
            average_length = total_characters / get_total_words(text)
        else:
            average_length = 0

        return average_length

    #EXPORTING TO EXCEL FILE
    def export_data(data):
      df = pd.DataFrame(data)
      df.to_excel('Output Data Structure.xlsx', index=False)

data = []
i = 2
with open("URL_ID.txt", 'w') as file:
    for url in dataset['URL']:
        item = {}
        with open("Temp.txt", 'w') as tempfile:
            r = requests.get(url)
            HtmlContent = r.content
            soup = BeautifulSoup(HtmlContent, 'html.parser')
            title = str(soup.find_all('title'))
            paras = str(soup.find_all('p'))
            heading1 = str(soup.find_all('h1'))
            heading3 = str(soup.find_all('h3'))
            Sub_head = str(soup.find_all('ol'))
            file.write(title)
            file.write(paras)
            file.write(heading1)
            file.write(Sub_head)
            file.write(heading3)
            tempfile.write(title)
            tempfile.write(paras)
            tempfile.write(heading1)
            tempfile.write(Sub_head)
            tempfile.write(heading3)
        item['POSITIVE SCORE'] = get_positive_score(fetch_text_from_file('Temp.txt'))
        item['NEGATIVE SCORE'] = get_negative_score(fetch_text_from_file('Temp.txt'))
        item['POLARITY SCORE'] = get_polarity_score(fetch_text_from_file('Temp.txt'))
        item['SUBJECTIVITY SCORE'] = get_subjectivity_score(fetch_text_from_file('Temp.txt'))
        item['AVERAGE SENTECE LENGTH'] = get_average_sentence_length(fetch_text_from_file('Temp.txt'))
        item['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words(fetch_text_from_file('Temp.txt'))
        item['FOG INDEX'] = calculate_fog_index(fetch_text_from_file('Temp.txt'))
        item['AVERAGE NUMBER OF WORDS PER SENTECE'] = average_words_per_sentence(fetch_text_from_file('Temp.txt'))
        item['COMPLEX WORD COUNT'] = count_complex_words(fetch_text_from_file('Temp.txt'))
        item['WORD COUNT'] = get_word_count(fetch_text_from_file('Temp.txt'))
        item['SYLLABLE PER WORD'] = get_syllables_per_word(fetch_text_from_file('Temp.txt'))
        item['PERSONAL PRONOUN'] = find_personal_pronouns(fetch_text_from_file('Temp.txt'))
        item['AVERAGE WORD LENGTH'] = average_word_length(fetch_text_from_file('Temp.txt'))
        data.append(item)
        export_data(data)
        erase_file_contents('Temp.txt')

        if i < 149:
            i += 1

# Close the main file outside the loop
file.close()